{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env japan-nlp)",
      "name": "py-dku-venv-japan-nlp",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.8",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_trip_advisor_TF_IDF",
    "createdOn": 1622734648273,
    "hide_input": false,
    "customFields": {},
    "creator": "admin",
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom gensim import corpora\nfrom gensim import models\nimport MeCab\nfrom gensim.models import word2vec"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/mmiyazaki/dataiku/Design/DATA_DIR/code-envs/python/japan-nlp/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\nscipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n  _deprecated()\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nyour_trip_advisor \u003d dataiku.Dataset(\"trip_advisor_clustered_prepared\")\ndf \u003d your_trip_advisor.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 8,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "list_vocabs \u003d df[df[\u0027cluster_labels\u0027].isin([\u0027接客などのサービス\u0027, \u0027具材・素材・味\u0027])][\u0027words_concat\u0027].values"
      ],
      "outputs": []
    },
    {
      "execution_count": 11,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ramen_word \u003d list_vocabs[0].split(\",\") + list_vocabs[1].split(\",\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 13,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ramen_dictionary_path \u003d dataiku.Folder(\"POe5uF4H\").get_path() + \"/ramen_dictionary\"\ndictionary \u003d corpora.Dictionary.load(ramen_dictionary_path)\ncorpus \u003d list(map(dictionary.doc2bow, ramen_word))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-13-7e0f08a97ddb\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mramen_dictionary_path\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mdataiku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POe5uF4H\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/ramen_dictionary\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mramen_dictionary_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mramen_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/dataiku/Design/DATA_DIR/code-envs/python/japan-nlp/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
          ],
          "ename": "TypeError"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wakati_folder \u003d dataiku.Folder(\"0kM5kXKs\").get_path()\ntagger_path \u003d \u0027-Owakati -d \u0027 + wakati_folder"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tagger \u003d MeCab.Tagger(tagger_path)#タグはMeCab.Tagger（neologd辞書）を使用\ntagger.parse(\u0027\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def tokenize_ja(text, lower):\n    node \u003d tagger.parseToNode(str(text))\n    while node:\n        if lower and node.feature.split(\u0027,\u0027)[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n            yield node.surface.lower()\n        node \u003d node.next\ndef tokenize(content, token_min_len, token_max_len, lower):\n    return [\n        str(token) for token in tokenize_ja(content, lower)\n        if token_min_len \u003c\u003d len(token) \u003c\u003d token_max_len and not token.startswith(\u0027_\u0027)\n    ]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wakati_ramen_text \u003d []\nfor i in df[\u0027jp\u0027]:\n    txt \u003d tokenize(i, 2, 10000, True)\n    wakati_ramen_text.append(txt)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#[w for w in sublist in wakati_ramen_text]\nvocab \u003d [w for sublist in wakati_ramen_text for w in sublist]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "words \u003d []\nvectors \u003d []\nfor word in vocab:\n    try:\n        vector \u003d ramen_model.wv[word]\n        words.append(word)\n        vectors.append(vector)\n    except KeyError:\n        None"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocab_df \u003d pd.DataFrame(vectors)\nvocab_df[\u0027words\u0027] \u003d words"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trip_advisor_tf_idf \u003d dataiku.Dataset(\"trip_advisor_vocabs\")\ntrip_advisor_tf_idf.write_with_schema(vocab_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#ramen_dictionary_path \u003d dataiku.Folder(\"POe5uF4H\").get_path() + \"/ramen_dictionary\"\n#dictionary \u003d corpora.Dictionary.load(ramen_dictionary_path)\n#corpus \u003d list(map(dictionary.doc2bow, reviews_concat))"
      ],
      "outputs": []
    }
  ]
}