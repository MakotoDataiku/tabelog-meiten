{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env japan-nlp)",
      "name": "py-dku-venv-japan-nlp",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.8",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_trip_advisor_TF_IDF",
    "createdOn": 1622734648273,
    "hide_input": false,
    "customFields": {},
    "creator": "admin",
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom gensim import corpora\nfrom gensim import models\nimport MeCab\nfrom gensim.models import word2vec"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/mmiyazaki/dataiku/Design/DATA_DIR/code-envs/python/japan-nlp/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\nscipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n  _deprecated()\n"
        }
      ]
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nyour_trip_advisor \u003d dataiku.Dataset(\"trip_advisor_clustered_prepared\")\ndf \u003d your_trip_advisor.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 19,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tfidf_path \u003d dataiku.Folder(\"tMMk2S0T\").get_path() + \"/tf_idf\""
      ],
      "outputs": []
    },
    {
      "execution_count": 8,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "list_vocabs \u003d df[df[\u0027cluster_labels\u0027].isin([\u0027接客などのサービス\u0027, \u0027具材・素材・味\u0027])][\u0027words_concat\u0027].values"
      ],
      "outputs": []
    },
    {
      "execution_count": 11,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ramen_word \u003d list_vocabs[0].split(\",\") + list_vocabs[1].split(\",\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 17,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ramen_dictionary_path \u003d dataiku.Folder(\"POe5uF4H\").get_path() + \"/ramen_dictionary\"\ndictionary \u003d corpora.Dictionary.load(ramen_dictionary_path)\ncorpus \u003d list(map(dictionary.doc2bow, [ramen_word]))"
      ],
      "outputs": []
    },
    {
      "execution_count": 18,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corpus"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "[[(2, 1),\n  (3, 1),\n  (6, 1),\n  (12, 2),\n  (23, 1),\n  (25, 1),\n  (34, 3),\n  (36, 1),\n  (41, 2),\n  (52, 2),\n  (56, 2),\n  (62, 1),\n  (63, 5),\n  (64, 1),\n  (79, 2),\n  (87, 1),\n  (90, 11),\n  (126, 2),\n  (132, 6),\n  (135, 1),\n  (139, 1),\n  (170, 15),\n  (187, 1),\n  (194, 1),\n  (206, 2),\n  (213, 3),\n  (220, 1),\n  (222, 1),\n  (228, 1),\n  (246, 1),\n  (256, 1),\n  (257, 1),\n  (283, 1),\n  (290, 1),\n  (291, 3),\n  (320, 1),\n  (325, 6),\n  (327, 1),\n  (330, 1),\n  (332, 5),\n  (337, 1),\n  (356, 1),\n  (389, 1),\n  (392, 2),\n  (412, 1),\n  (413, 4),\n  (420, 1),\n  (424, 6),\n  (425, 1),\n  (448, 1),\n  (459, 3),\n  (462, 3),\n  (464, 4),\n  (469, 1),\n  (489, 1),\n  (504, 2),\n  (540, 2),\n  (623, 1),\n  (627, 1),\n  (655, 1),\n  (680, 1),\n  (681, 1),\n  (719, 8),\n  (758, 1),\n  (782, 3),\n  (819, 2),\n  (842, 1),\n  (853, 1),\n  (881, 2),\n  (1008, 1),\n  (1015, 1),\n  (1025, 1),\n  (1095, 1),\n  (1117, 1),\n  (1189, 1),\n  (1312, 1),\n  (1372, 1),\n  (1394, 1),\n  (1406, 1),\n  (1426, 2),\n  (1471, 1),\n  (1487, 1),\n  (1512, 7),\n  (1561, 7),\n  (1590, 1),\n  (1641, 1),\n  (1672, 1),\n  (1703, 1),\n  (1715, 1),\n  (1783, 1),\n  (1842, 1),\n  (1917, 1),\n  (1968, 1),\n  (1970, 1),\n  (2126, 1),\n  (2184, 1),\n  (2260, 1),\n  (2278, 2),\n  (2300, 2),\n  (2355, 5),\n  (2428, 2),\n  (2441, 1),\n  (2465, 1),\n  (2598, 1),\n  (2689, 13),\n  (2736, 1),\n  (2953, 1),\n  (3080, 1)]]"
          },
          "execution_count": 18
        }
      ]
    },
    {
      "execution_count": 20,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "models.load(tfidf_path)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "evalue": "module \u0027gensim.models\u0027 has no attribute \u0027load\u0027",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-20-01e9054306d7\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module \u0027gensim.models\u0027 has no attribute \u0027load\u0027"
          ],
          "ename": "AttributeError"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wakati_folder \u003d dataiku.Folder(\"0kM5kXKs\").get_path()\ntagger_path \u003d \u0027-Owakati -d \u0027 + wakati_folder"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tagger \u003d MeCab.Tagger(tagger_path)#タグはMeCab.Tagger（neologd辞書）を使用\ntagger.parse(\u0027\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def tokenize_ja(text, lower):\n    node \u003d tagger.parseToNode(str(text))\n    while node:\n        if lower and node.feature.split(\u0027,\u0027)[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n            yield node.surface.lower()\n        node \u003d node.next\ndef tokenize(content, token_min_len, token_max_len, lower):\n    return [\n        str(token) for token in tokenize_ja(content, lower)\n        if token_min_len \u003c\u003d len(token) \u003c\u003d token_max_len and not token.startswith(\u0027_\u0027)\n    ]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "wakati_ramen_text \u003d []\nfor i in df[\u0027jp\u0027]:\n    txt \u003d tokenize(i, 2, 10000, True)\n    wakati_ramen_text.append(txt)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#[w for w in sublist in wakati_ramen_text]\nvocab \u003d [w for sublist in wakati_ramen_text for w in sublist]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "words \u003d []\nvectors \u003d []\nfor word in vocab:\n    try:\n        vector \u003d ramen_model.wv[word]\n        words.append(word)\n        vectors.append(vector)\n    except KeyError:\n        None"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocab_df \u003d pd.DataFrame(vectors)\nvocab_df[\u0027words\u0027] \u003d words"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trip_advisor_tf_idf \u003d dataiku.Dataset(\"trip_advisor_vocabs\")\ntrip_advisor_tf_idf.write_with_schema(vocab_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#ramen_dictionary_path \u003d dataiku.Folder(\"POe5uF4H\").get_path() + \"/ramen_dictionary\"\n#dictionary \u003d corpora.Dictionary.load(ramen_dictionary_path)\n#corpus \u003d list(map(dictionary.doc2bow, reviews_concat))"
      ],
      "outputs": []
    }
  ]
}