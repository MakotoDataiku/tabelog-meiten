{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env japan-nlp)",
      "name": "py-dku-venv-japan-nlp",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.8",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_TF_IDF_scores",
    "createdOn": 1622559061978,
    "hide_input": false,
    "modifiedBy": "admin",
    "customFields": {},
    "creator": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 12,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim import corpora\nfrom gensim import models"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/Users/mmiyazaki/dataiku/Design/DATA_DIR/code-envs/python/japan-nlp/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\nscipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n  _deprecated()\n"
        }
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nramen_clusters_named \u003d dataiku.Dataset(\"ramen_clusters_named\")\ndf \u003d ramen_clusters_named.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "list_vocabs \u003d df[df[\u0027cluster_labels\u0027].isin([\u0027接客などのサービス\u0027, \u0027具材・素材・味\u0027])][\u0027words_concat\u0027].values"
      ],
      "outputs": []
    },
    {
      "execution_count": 4,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ramen_word \u003d list_vocabs[0].split(\",\") + list_vocabs[1].split(\",\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 7,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "folder_path \u003d dataiku.Folder(\"aLTWBozg\").get_path()\ntext_path \u003d folder_path + \"/ramen_corpus.txt\""
      ],
      "outputs": []
    },
    {
      "execution_count": 9,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f \u003d open(text_path,\u0027r\u0027,encoding\u003d\"utf-8\")\ntrainings \u003d []"
      ],
      "outputs": []
    },
    {
      "execution_count": 10,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i,data in enumerate(f):\n    word \u003d data.replace(\"\u0027\",\u0027\u0027).replace(\u0027[\u0027,\u0027\u0027).replace(\u0027]\u0027,\u0027\u0027).replace(\u0027 \u0027,\u0027\u0027).replace(\u0027\\n\u0027,\u0027\u0027).split(\",\")\n    trainings.append([i for i in word if i in ramen_word])"
      ],
      "outputs": []
    },
    {
      "execution_count": 31,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(trainings)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "metadata": {},
          "data": {
            "text/plain": "211"
          },
          "execution_count": 31
        }
      ]
    },
    {
      "execution_count": 13,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dictionary \u003d corpora.Dictionary(trainings)"
      ],
      "outputs": []
    },
    {
      "execution_count": 15,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corpus \u003d list(map(dictionary.doc2bow,trainings))"
      ],
      "outputs": []
    },
    {
      "execution_count": 16,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_model \u003d models.TfidfModel(corpus)"
      ],
      "outputs": []
    },
    {
      "execution_count": 17,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corpus_tfidf \u003d test_model[corpus]"
      ],
      "outputs": []
    },
    {
      "execution_count": 19,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# id-\u003e単語へ変換\ntexts_tfidf \u003d [] # id -\u003e 単語表示に変えた文書ごとのTF-IDF\nfor doc in corpus_tfidf:\n    text_tfidf \u003d []\n    for word in doc:\n        text_tfidf.append([dictionary[word[0]],word[1]])\n    texts_tfidf.append(text_tfidf)"
      ],
      "outputs": []
    },
    {
      "execution_count": 28,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from operator import itemgetter\n\ntexts_tfidf_sorted_top20 \u003d [] \n\n#TF-IDF値を高い順に並び替え上位単語20個に絞る。\nfor i in range(len(texts_tfidf)):\n    soted \u003d sorted(texts_tfidf[i], key\u003ditemgetter(1),reverse\u003dTrue)\n    soted_top20 \u003d soted[:20]\n    word_list \u003d []\n    for k in range(len(soted_top20)):\n        word \u003d soted_top20[k][0]\n        word_list.append(word)\n    texts_tfidf_sorted_top20.append(word_list)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute recipe outputs from inputs\n# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n\ntf_IDF_scores_df \u003d ramen_clusters_named_df # For this sample code, simply copy input to output\n\n\n# Write recipe outputs\ntf_IDF_scores \u003d dataiku.Dataset(\"ramen_by_store_name\").get_dataframe()"
      ],
      "outputs": []
    }
  ]
}