{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env japan-nlp)",
      "name": "py-dku-venv-japan-nlp",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.6.8",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "compute_m9JZdV7b",
    "createdOn": 1622485642198,
    "hide_input": false,
    "customFields": {},
    "creator": "admin",
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read recipe inputs\nraw_ramen \u003d dataiku.Dataset(\"raw_ramen\")\nraw_ramen_df \u003d raw_ramen.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\nimport pandas as pd\nimport pickle\nfrom gensim.models import word2vec\nimport MeCab\n\ntagger \u003d MeCab.Tagger(\u0027-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\u0027)#タグはMeCab.Tagger（neologd辞書）を使用\n\ntagger.parse(\u0027\u0027)\ndef tokenize_ja(text, lower):\n    node \u003d tagger.parseToNode(str(text))\n    while node:\n        if lower and node.feature.split(\u0027,\u0027)[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n            yield node.surface.lower()\n        node \u003d node.next\ndef tokenize(content, token_min_len, token_max_len, lower):\n    return [\n        str(token) for token in tokenize_ja(content, lower)\n        if token_min_len \u003c\u003d len(token) \u003c\u003d token_max_len and not token.startswith(\u0027_\u0027)\n    ]\n\n\n\n#学習データの読み込み\n\ndf \u003d pd.read_csv(\u0027../output/tokyo_ramen_review.csv\u0027)\ndf_ramen \u003d df.groupby([\u0027store_name\u0027,\u0027score\u0027,\u0027review_cnt\u0027])[\u0027review\u0027].apply(list).apply(\u0027 \u0027.join).reset_index().sort_values(\u0027score\u0027, ascending\u003dFalse)\n\n#コーパス作成\nwakati_ramen_text \u003d []\nfor i in df_ramen[\u0027review\u0027]:\n    txt \u003d tokenize(i, 2, 10000, True)\n    wakati_ramen_text.append(txt)\n↓↓↓↓↓↓↓ あなたの記事の内容\nnp.savetxt(\"../work/ramen_corpus.txt\", wakati_ramen_text,fmt\u003d\u0027%s\u0027, delimiter\u003d\u0027,\u0027)\n───────\nnp.savetxt(\"../work/ramen_corpus.txt\", wakati_ramen_text, fmt \u003d \u0027%s\u0027, delimiter \u003d \u0027,\u0027)\n↑↑↑↑↑↑↑ 編集リクエストの内容\n# モデル作成\n↓↓↓↓↓↓↓ あなたの記事の内容\nword2vec_ramen_model \u003d word2vec.Word2Vec(wakati_ramen_text,sg\u003d1,size\u003d100, window\u003d5,min_count\u003d5,iter\u003d100,workers\u003d3)\n───────\nword2vec_ramen_model \u003d word2vec.Word2Vec(wakati_ramen_text, sg \u003d 1, size \u003d 100, window \u003d 5, min_count \u003d 5, iter \u003d 100, workers \u003d 3)\n↑↑↑↑↑↑↑ 編集リクエストの内容\n#sg（0: CBOW, 1: skip-gram）,size（ベクトルの次元数）,window（学習に使う前後の単語数）,min_count（n回未満登場する単語を破棄）,iter（トレーニング反復回数）\n\n# モデルのセーブ\nword2vec_ramen_model.save(\"../model/word2vec_ramen_model.model\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write recipe outputs\nword2vec \u003d dataiku.Folder(\"m9JZdV7b\")\nword2vec_info \u003d word2vec.get_info()"
      ],
      "outputs": []
    }
  ]
}