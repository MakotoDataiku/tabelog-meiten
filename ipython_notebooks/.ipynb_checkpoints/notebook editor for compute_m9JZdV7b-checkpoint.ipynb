{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (env japan-nlp)",
      "language": "python",
      "name": "py-dku-venv-japan-nlp"
    },
    "associatedRecipe": "compute_m9JZdV7b",
    "creator": "admin",
    "createdOn": 1622799655142,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import dataiku\n",
        "import pandas as pd, numpy as np\n",
        "from dataiku import pandasutils as pdu\n",
        "import pickle\n",
        "from gensim.models import word2vec\n",
        "import MeCab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Read recipe inputs\n",
        "raw_ramen \u003d dataiku.Dataset(\"raw_ramen\")\n",
        "df \u003d raw_ramen.get_dataframe()\n",
        "df_ramen \u003d df.groupby([\u0027store_name\u0027,\u0027score\u0027,\u0027review_cnt\u0027])[\u0027review\u0027].apply(list).apply(\u0027 \u0027.join).reset_index().sort_values(\u0027score\u0027, ascending\u003dFalse)\n",
        "\n",
        "w2v_folder \u003d dataiku.Folder(\"m9JZdV7b\").get_path()\n",
        "text_folder \u003d dataiku.Folder(\"aLTWBozg\").get_path()\n",
        "wakati_folder \u003d dataiku.Folder(\"0kM5kXKs\").get_path()\n",
        "tagger_path \u003d \u0027-Owakati -d \u0027 + wakati_folder\n",
        "tagger_path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "df_ramen.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "tagger \u003d MeCab.Tagger(tagger_path)#タグはMeCab.Tagger（neologd辞書）を使用\n",
        "tagger.parse(\u0027\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "def tokenize_ja(text, lower):\n",
        "    node \u003d tagger.parseToNode(str(text))\n",
        "    while node:\n",
        "        if lower and node.feature.split(\u0027,\u0027)[0] in [\"名詞\",\"形容詞\"]:#分かち書きで取得する品詞を指定\n",
        "            yield node.surface.lower()\n",
        "        node \u003d node.next\n",
        "def tokenize(content, token_min_len, token_max_len, lower):\n",
        "    return [\n",
        "        str(token) for token in tokenize_ja(content, lower)\n",
        "        if token_min_len \u003c\u003d len(token) \u003c\u003d token_max_len and not token.startswith(\u0027_\u0027)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "#コーパス作成\n",
        "wakati_ramen_text \u003d []\n",
        "for i in df_ramen[\u0027review\u0027]:\n",
        "    txt \u003d tokenize(i, 2, 10000, True)\n",
        "    wakati_ramen_text.append(txt)\n",
        "np.savetxt(text_folder + \"/ramen_corpus.txt\", wakati_ramen_text, fmt \u003d \u0027%s\u0027, delimiter \u003d \u0027,\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# モデル作成\n",
        "word2vec_ramen_model \u003d word2vec.Word2Vec(wakati_ramen_text, sg \u003d 1, size \u003d 100, window \u003d 5, min_count \u003d 5, iter \u003d 100, workers \u003d 3)\n",
        "#sg（0: CBOW, 1: skip-gram）,size（ベクトルの次元数）,window（学習に使う前後の単語数）,min_count（n回未満登場する単語を破棄）,iter（トレーニング反復回数）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# モデルのセーブ\n",
        "word2vec_ramen_model.save(w2v_folder+\"/word2vec_ramen_model.model\")\n",
        "\n",
        "max_vocab \u003d 30000 #40000にしても結果は同じだった\n",
        "vocab \u003d list(word2vec_ramen_model.wv.vocab.keys())[:max_vocab]\n",
        "vectors \u003d [word2vec_ramen_model.wv[word] for word in vocab]\n",
        "\n",
        "vocab_df \u003d pd.DataFrame(vectors)\n",
        "vocab_df[\u0027words\u0027] \u003d vocab\n",
        "\n",
        "py_recipe_output \u003d dataiku.Dataset(\"ramen_vocab\")\n",
        "py_recipe_output.write_with_schema(vocab_df)"
      ]
    }
  ]
}